import streamlit as st
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph
from src.pipeline import graph  # Import the compiled graph from your orchestrator

# Configurazione iniziale dell'app Streamlit
st.set_page_config(
    page_title="Agentic RAG Chatbot",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.title("ü§ñ Agentic RAG Chatbot")
st.markdown(
    """
    Welcome to the **Agentic Retrieval-Augmented Generation (RAG) Chatbot**!
    Ask questions, and the chatbot will fetch relevant documents, analyze them, and generate concise responses.
    """
)

# Sidebar con istruzioni
with st.sidebar:
    st.header("How it works:")
    st.markdown(
        """
        1. Enter your query in the text box.
        2. The chatbot processes your query using an agentic pipeline.
        3. It retrieves documents, checks relevance, and generates an answer.
        4. Intermediate steps are shown in the output.
        """
    )
    st.markdown("---")
    st.markdown("### Example Queries:")
    st.markdown("- What does the GDPR regulate?")
    st.markdown("- What are the principles of GDPR?")
    st.markdown("- Who needs to comply with GDPR?")
    st.markdown("---")
    st.info("üí° Use this interface to explore the capabilities of the RAG pipeline!")

# Input per la domanda
query = st.text_input("üí¨ Enter your question:", value="")

# Spazi per l'output
st.markdown("### Intermediate Steps:")
intermediate_steps = st.empty()

st.markdown("### Final Answer:")
final_answer = st.empty()

# Messaggio dinamico per il caricamento
loading_message = st.empty()

# Esegui il flusso RAG
if st.button("Submit Query") and query.strip():
    loading_message.info("Processing your query... Please wait.")
    inputs = {"messages": [HumanMessage(content=query)]}
    outputs = []

    try:
        # Esegui la pipeline e mostra i passi intermedi
        for output in graph.stream(inputs):
            for key, value in output.items():
                outputs.append((key, value))
                intermediate_steps.markdown(
                    f"#### Node: `{key}`\n```json\n{value}\n```", unsafe_allow_html=True
                )

        # Estrai la risposta finale
        if outputs:
            final = outputs[-1]
            response = final[1].get("response", None)
            if response:
                final_answer.markdown(f"**Response:** {response}")
            else:
                final_answer.error("‚ö†Ô∏è Unable to generate a response.")
        else:
            final_answer.error("‚ö†Ô∏è No output generated by the pipeline.")

    except Exception as e:
        # Gestione degli errori: Mostra il messaggio e interrompi
        final_answer.error(f"An error occurred: {e}")

    finally:
        # Rimuovi il messaggio di caricamento
        loading_message.empty()
